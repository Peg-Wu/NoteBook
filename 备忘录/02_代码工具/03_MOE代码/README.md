# é¢„æµ‹äººç±»RNAä¸­2'-O-ç”²åŸºåŒ–(2OM)ä½ç‚¹

- åŸºäºŽMixture Of Experts(MOE)æ¨¡åž‹
  - Expertä¸“å®¶æ¨¡åž‹ä»…ç”¨äº†ä¸¤å±‚CNNå’Œå…¨è¿žæŽ¥å±‚ï¼Œ**æ¨¡åž‹å¾…å®Œå–„ä¸­...**
  - Gateé—¨æŽ§æ¨¡åž‹ä»…ç”¨äº†ä¸¤å±‚å…¨è¿žæŽ¥å±‚ï¼Œ**æ¨¡åž‹å¾…å®Œå–„ä¸­...**
  - è¯¥æ¨¡åž‹åŒ…å«4ä¸ªExpertsï¼Œæ¯ä¸ªExpertæ¨¡åž‹æž¶æž„ç›®å‰æ˜¯ä¸€è‡´çš„ï¼Œ**è¿™4ä¸ªä¸“å®¶åˆ†åˆ«å¤„ç†A2OMã€G2OMã€C2OMå’ŒU2OMæ•°æ®**
- è®­ç»ƒè¯·è¿è¡Œ`Train.py`
- æµ‹è¯•è¯·è¿è¡Œ`Test.py`
- æœ¬é¡¹ç›®æä¾›äº†MOEæ¨¡åž‹è®­ç»ƒå¯è§†åŒ–ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šç”Ÿæˆ`./logs`æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨`Tensorboard`æŸ¥çœ‹ï¼

## ðŸŽ¨è®­ç»ƒç­–ç•¥è¯´æ˜Ž

1. ä¸“å®¶æ¨¡åž‹åœ¨è®­ç»ƒXä¸ªepochåŽï¼Œå¦‚æžœ`è®­ç»ƒæŸå¤±`æ²¡æœ‰é™ä½Žï¼Œåˆ™æå‰åœæ­¢è®­ç»ƒ
2. MOEæ¨¡åž‹åœ¨è®­ç»ƒXä¸ªepochåŽï¼Œå¦‚æžœ`éªŒè¯æŸå¤±`æ²¡æœ‰é™ä½Žï¼Œåˆ™æå‰åœæ­¢è®­ç»ƒ
3. ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒä¸“å®¶æ¨¡åž‹ï¼Œ`ä¸“å®¶æ¨¡åž‹æ²¡æœ‰éªŒè¯é›†`ï¼Œåªè¿›è¡Œè®­ç»ƒï¼Œæ ¹æ®è®­ç»ƒæŸå¤±éšæ—¶ç»ˆæ­¢
4. ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒMOEæ¨¡åž‹ï¼Œ`MOEæ¨¡åž‹æœ‰éªŒè¯é›†`ï¼Œå¹¶è®°å½•äº†Accuracyã€MCCã€Precisionã€F1ã€Spã€Snç­‰æŒ‡æ ‡

## æµ‹è¯•é›†æ³›åŒ–æƒ…å†µ

- Model1: Expertæ¨¡åž‹åŸºäºŽ**CNN-Transformer**

```python
import torch
from torch import nn

def conv1d_k1_block(in_channels, out_channels, padding=0, stride=1):
    return nn.Sequential(nn.Conv1d(in_channels, out_channels, 1, stride, padding),
                         nn.BatchNorm1d(out_channels), nn.ReLU())

class CancelOut(nn.Module):
    '''
    CancelOut Layer
    x - an input data (vector, matrix, tensor)
    '''
    def __init__(self, inp, *kargs, **kwargs):
        super(CancelOut, self).__init__()
        self.weights = nn.Parameter(torch.zeros(inp, requires_grad=True) + 4)

    def forward(self, x):
        return (x * torch.sigmoid(self.weights.float()))

class Expert(nn.Module):
    def __init__(self):
        super(Expert, self).__init__()
        self.conv3 = nn.Sequential(nn.Conv1d(4, 64, 3, 1, 1), nn.BatchNorm1d(64), nn.ReLU(),
                                   conv1d_k1_block(64, 32))
        self.conv5 = nn.Sequential(nn.Conv1d(4, 64, 5, 1, 2), nn.BatchNorm1d(64), nn.ReLU(),
                                   conv1d_k1_block(64, 32))
        self.conv7 = nn.Sequential(nn.Conv1d(4, 64, 7, 1, 3), nn.BatchNorm1d(64), nn.ReLU(),
                                   conv1d_k1_block(64, 32))
        self.conv9 = nn.Sequential(nn.Conv1d(4, 64, 9, 1, 4), nn.BatchNorm1d(64), nn.ReLU(),
                                   conv1d_k1_block(64, 32))
        self.tf_encoder_layer = nn.TransformerEncoderLayer(128, 8, batch_first=True)
        self.tf_encoder = nn.TransformerEncoder(self.tf_encoder_layer, 3)
        self.dense = nn.Sequential(nn.Linear(128, 32), nn.BatchNorm1d(32), nn.ReLU(), nn.Linear(32, 2))

    def forward(self, X):
        X = X.permute(0, 2, 1)  # (batch, 4, 41)
        # å°†one_hotä½œä¸ºç‰¹å¾ç»´è¿›è¡Œå·ç§¯
        X = torch.cat([self.conv3(X), self.conv5(X), self.conv7(X), self.conv9(X)], dim=1)  # (batch, 128, 41)
        X = X.permute(0, 2, 1)  # (batch_size, 41, 128)
        X = self.tf_encoder(X)
        # Mean pooling
        X = torch.mean(X, dim=1)
        X = self.dense(X)
        return X

class Gate(nn.Module):
    def __init__(self, num_experts=4):
        super(Gate, self).__init__()
        self.num_experts = num_experts
        self.dense = nn.Sequential(
            nn.Flatten(),
            CancelOut(41*4),
            nn.Linear(41*4, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 4),
            nn.Softmax(dim=1))

    def forward(self, X):
        return self.dense(X)

class MOE(nn.Module):
    def __init__(self, trained_experts: list):
        super(MOE, self).__init__()
        self.experts = nn.ModuleList(trained_experts)
        self.num_experts = len(trained_experts)
        self.gate = Gate(num_experts=4)

    def forward(self, X):
        weights = self.gate(X)
        # åŽä¸¤ç»´ï¼šæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªä¸“å®¶çš„é¢„æµ‹ç»“æžœ
        outputs = torch.stack([expert(X) for expert in self.experts], dim=2)
        weights = weights.unsqueeze(1).expand_as(outputs)
        return torch.sum(outputs * weights, dim=2)
```

> MOE:
> Accuracy: 0.8095628413088627
> Precision: 0.8114348538694696
> F1: 0.8089887638232424
> MCC: 0.6191368675842226
> Sn: 0.8065573766084385
> Sp: 0.8125683055669025

