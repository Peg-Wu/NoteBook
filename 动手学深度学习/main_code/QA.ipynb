{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"关于K折交叉验证\"\"\"\n",
    "# K折交叉验证，确定超参数后：\n",
    "# 做法1：用最后确定的超参数重新在整个数据集上训练一遍，重新划分数据集\n",
    "# 做法2：任意取某一折的模型，或者取精度最好的那一折（模型不用再重复训练了）\n",
    "# 做法3：所有的k个模型全部保留，在预测的时候把样本丢进这k个模型中，预测结果取均值（预测的代价高，好处是能增加模型的稳定性）\n",
    "# \"\"\"一般深度学习数据量比较大的话不会做交叉验证，机器学习中比较常见，K一般取5或10\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'关于VC维'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"关于VC维\"\"\"\n",
    "# VC维是指模型能记住的最大的样本量，无论数据怎么标号，模型都能很好的拟合这些样本"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"关于处理不平衡数据集\"\"\"\n",
    "# 如果正负类比例是9：1，训练时我们可以对loss进行加权，少的类赋予更大的权重"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"关于权重衰退\"\"\"\n",
    "# 在Loss损失的后面加了一个L2正则项，λ/2 * w的L2范数的平方\n",
    "# 对这个Loss求关于w的梯度可以发现：在随机梯度下降的时候，相当于先对w进行了一定尺度(1-lr*λ)的缩小，再进行移动\n",
    "# 限制模型在很小的参数范围内取参数，是一种防止过拟合，控制模型复杂度的方式"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"batch_size\"\"\"\n",
    "# 通常来说，如果一个图片集有n个类，批量大小最好不要超过10*n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "GRU:(重置门-更新门)\n",
    "$$\n",
    "$$\n",
    "R_t=\\sigma \\left( X_tW_{xr}+H_{t-1}W_{hr}+b_r \\right)\n",
    "$$\n",
    "$$\n",
    "Z_t=\\sigma \\left( X_tW_{xz}+H_{t-1}W_{hz}+b_z \\right)\n",
    "$$\n",
    "$$\n",
    "\\tilde{H}_t=\\tan\\text{h}\\left( X_tW_{xh}+\\left( R_t*H_{t-1} \\right) W_{hh}+b_h \\right)\n",
    "$$\n",
    "$$\n",
    "H_t=Z_t*H_{t-1}+\\left( 1-Z_t \\right) *\\tilde{H}_t\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GRU和LSTM如果使用梯度剪裁，一般theta采用1，5，10\n",
    "# 双向循环神经网络通常用来对序列抽取特征、填空，而不是预测未来，正向和反向的权重是不一样的！"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "LSTM:(输入门-遗忘门-输出门)\n",
    "$$\n",
    "$$\n",
    "I_t=\\sigma \\left( X_tW_{xi}+H_{t-1}W_{hi}+b_i \\right)\n",
    "$$\n",
    "$$\n",
    "F_t=\\sigma \\left( X_tW_{xf}+H_{t-1}W_{hf}+b_f \\right)\n",
    "$$\n",
    "$$\n",
    "O_t=\\sigma \\left( X_tW_{xo}+H_{t-1}W_{ho}+b_o \\right)\n",
    "$$\n",
    "$$\n",
    "\\tilde{C}_t=\\tan\\text{h}\\left( X_tW_{xc}+H_{t-1}W_{hc}+b_c \\right)\n",
    "$$\n",
    "$$\n",
    "C_t=F_t*C_{t-1}+I_t*\\tilde{C}_t\n",
    "$$\n",
    "$$\n",
    "H_t=O_t*\\tan\\text{h}\\left( C_t \\right)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"两个版本的BERT\"\"\"\n",
    "# BERT针对微调设计\n",
    "# BERT是只有编码器的Transformer，在大规模数据集上训练 > 3B 词\n",
    "# Base: #blocks=12, #hidden size=768, #heads=12, #parameters=110M\n",
    "# Large: #blocks=24, #hidden size=1024, #heads=16, #parameters=340M\n",
    "\n",
    "\"\"\"对输入的修改：\"\"\"\n",
    "# 每个样本是一个**句子对**\n",
    "# 加入额外的片段嵌入\n",
    "# 位置编码可学习\n",
    "# Token Embeddings --> Segment Embeddings --> Position Embeddings\n",
    "\n",
    "# Transformer的编码器是双向，标准语言模型要求单向\n",
    "# 带掩码的语言模型每次随机 (15%概率) 将一些词元换成<mask>\n",
    "\n",
    "\"\"\"BERT预训练任务1：带掩码的语言模型\"\"\"\n",
    "# 因为微调任务中不出现<mask>:\n",
    "    # 80%概率下，将选中的词元变成<mask>\n",
    "    # 10%概率下，换成一个随机词元\n",
    "    # 10%概率下，保持原有的词元\n",
    "\n",
    "\"\"\"BERT预训练任务2：下一句子预测\"\"\"\n",
    "# 预测一个句子对中两个句子是不是相邻\n",
    "# 训练样本中：\n",
    "    # 50%概率选择相邻句子对：<cls>this movie is great<sep>i like it<sep>\n",
    "    # 50%概率选择随机句子对：<cls>this movie is great<sep>hello world<sep>\n",
    "# 将<cls>对应的输出放到一个全连接层来预测"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}